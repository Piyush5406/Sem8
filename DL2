------------------Part 1 OCR
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import pandas as pd
------------------------------------------------------
data_path = 'D:\Downloads\letter-recognition.data'
columns = ['letter'] + [f'feature_{i}' for i in range(16)]
df = pd.read_csv(data_path, names=columns)

label_encoder = LabelEncoder()
df['target'] = label_encoder.fit_transform(df['letter'])
X = df.drop(['letter', 'target'], axis=1)
y = df['target']

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
------------------------------------------------------------
# Build the deep neural network model
model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(16,)))
model.add(Dense(64, activation='relu'))
model.add(Dense(26, activation='softmax'))  # 26 classes for letters

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_accuracy}')
----------------------------------------------------------------
# Given data
import numpy as np
new_data = np.array([4, 7, 5, 5, 4, 6, 7, 3, 7, 11, 8, 9, 3, 8, 4, 8]).reshape(1, -1)

# Use the model to make predictions
predictions = model.predict(new_data)
print(predictions)
# Display the predictions
predicted_class = np.argmax(predictions)
print(f'The predicted class is: {predicted_class}')
---------------------------------------------------------------
class_mapping = {
    0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J',
    10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T',
    20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z'
}

# Display the predicted class using the mapping
predicted_letter = class_mapping[predicted_class]
print(f'The predicted class is: {predicted_class}, which corresponds to the letter: {predicted_letter}')
-------------------------------------------------------------------
#dummy = 4, 7, 5, 5, 4, 6, 7, 3, 7, 11, 8, 9, 3, 8, 4, 8

# Take input from the user
user_input = input("Enter values for the 17 features separated by commas: ")
user_input_list = [int(x) for x in user_input.split(',')]

# Convert the user input to a NumPy array
new_data = np.array(user_input_list).reshape(1, -1)

# Use the model to make predictions
predictions = model.predict(new_data)
#print(predictions)
# Display the predictions
predicted_class = np.argmax(predictions)
print(f'\nThe predicted class is: {predicted_class} i.e. {class_mapping[predicted_class]}')
------------------------------------------------------------------
import tkinter as tk
import numpy as np

# Function to handle button click
def display_name_and_predict():
    # Get the name from the input field
    name = entry.get()
    
    user_input_list = [int(x) for x in user_input.split(',')]
    
    # Convert the user input to a NumPy array
    new_data = np.array(user_input_list).reshape(1, -1)

    # Use the model to make predictions
    predictions = model.predict(new_data)
    
    # Display the predictions
    predicted_class = np.argmax(predictions)
    
    # Display the name on the label
    label.config(text=f"Predicted: {class_mapping[predicted_class]}")
-----------------------------------------------------------
# Create the main window
window = tk.Tk()
window.title("Prediction App")

# Create an input field
entry = tk.Entry(window, width=30)
entry.pack(pady=10)

# Create a button
button = tk.Button(window, text="Display Name and Predict", command=display_name_and_predict)
button.pack(pady=10)

# Create a display label
label = tk.Label(window, text="")
label.pack(pady=10)

# Run the Tkinter event loop
window.mainloop()
-----------------------------------------------------------------
----------------------------------------------------------------
Part 2 = IMDB

